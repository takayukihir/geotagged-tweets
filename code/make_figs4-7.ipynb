{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For i/o paths\n",
    "from pathlib import Path\n",
    "\n",
    "import m3code\n",
    "import analysis\n",
    "import models\n",
    "\n",
    "# For manipulating lines\n",
    "# from matplotlib.collections import LineCollection\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# For manipulating colors\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "import unicodedata\n",
    "\n",
    "FONTS_DIR = 'ipaexg00401'\n",
    "font_dirs = [FONTS_DIR]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "for fpath in font_files:\n",
    "    font_manager.fontManager.addfont(fpath)\n",
    "\n",
    "plt.rcParams['font.family'] = ['sans-serif', 'serif', 'IPAexGothic']\n",
    "plt.rcParams[\"font.sans-serif\"] = ['Helvetica']\n",
    "plt.rcParams['font.serif'] = ['Palatino']\n",
    "SMALL_FONT_SIZE = 10\n",
    "MEDIUM_FONT_SIZE = 12\n",
    "LARGE_FONT_SIZE = 14\n",
    "PANEL_LABEL_SIZE = 16\n",
    "PANEL_LABEL_FONT = 'DejaVu Sans'\n",
    "plt.rcParams.update({'font.size': MEDIUM_FONT_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "import json\n",
    "data_dir = Path().resolve().parent / 'scratch' / 'preprocessed_mcntlt7_selected'\n",
    "with (data_dir / 'file_names.json').open(mode='r') as f:\n",
    "    file_names_dict = json.load(f)\n",
    "all_file = data_dir / file_names_dict['all']['filename']\n",
    "\n",
    "figdir = Path().resolve().parent / 'figure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_vs_keyword(ax, data_all, data_word, \n",
    "                        style='scatter', cmap='Blues', linecolor='#482475', nlevels=3):\n",
    "    nonzero = data_word > 0\n",
    "    data_all_nz = data_all[nonzero]\n",
    "    data_word_nz = data_word[nonzero]\n",
    "    if style == 'scatter':\n",
    "        ax.scatter(data_all_nz, data_word_nz, s=1, c='k', marker='o', edgecolors='none', \n",
    "                    alpha=0.2, rasterized=True)\n",
    "        # ax.scatter(data_all_nz, data_word_nz / data_all_nz, s=1, c='k', marker='o', edgecolors='none',)\n",
    "    elif style == 'contour':\n",
    "        xmesh, ymesh, zmesh, dlevels = \\\n",
    "            models.data_contours(data_all_nz, data_word_nz, nlevels=nlevels, log_scale=True)\n",
    "        fillcolors = plt.colormaps[cmap]\n",
    "        ax.contourf(xmesh, ymesh, zmesh, \n",
    "            levels=np.append(dlevels, np.max(zmesh)), \n",
    "            colors=[fillcolors((i+1)/nlevels) for i in range(nlevels)], alpha=0.5)\n",
    "        ax.contour(xmesh, ymesh, zmesh, \n",
    "                levels=np.append(dlevels, np.max(zmesh)), \n",
    "                colors=linecolor, alpha=0.5, linewidths=0.6)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlim(left=1, right=1E6)\n",
    "    ax.set_ylim(bottom=0.8, top=5E4)\n",
    "\n",
    "def scatter_plot_all_vs_keyword_empirical(ax, all_kw_df, kw_en):\n",
    "    data_all = all_kw_df['tweetcount_all'].to_numpy()\n",
    "    data_word = all_kw_df['tweetcount_' + kw_en].to_numpy()\n",
    "    plot_all_vs_keyword(ax, data_all, data_word, style='scatter')\n",
    "    ax.minorticks_off()\n",
    "\n",
    "def contour_plot_all_vs_keyword_binomial(ax, all_kw_df, kw_en, rng, return_model=False, **params):\n",
    "    data_all = all_kw_df['tweetcount_all'].to_numpy()\n",
    "    data_word = all_kw_df['tweetcount_' + kw_en].to_numpy()\n",
    "    plot_all_vs_keyword(ax, data_all, data_word, \n",
    "                        style='contour', cmap='Blues', linecolor='#482475')\n",
    "    p_binomial = params['p_binomial']\n",
    "    model_all, model_word = \\\n",
    "        models.random_data_from_model(data_all, p_binomial[kw_en], rng, num_repeat=5)\n",
    "    plot_all_vs_keyword(ax, model_all, model_word, \n",
    "                        style='contour', cmap='Reds', linecolor='#D63230')\n",
    "    ax.minorticks_off()\n",
    "\n",
    "def contour_plot_all_vs_keyword_coreperi(ax, all_kw_df, kw_en, rng, return_model=False, **params):\n",
    "    data_all = all_kw_df['tweetcount_all'].to_numpy()\n",
    "    data_word = all_kw_df['tweetcount_' + kw_en].to_numpy()\n",
    "    plot_all_vs_keyword(ax, data_all, data_word, \n",
    "                        style='contour', cmap='Blues', linecolor='#482475')\n",
    "    distance = analysis.distance_from_center(all_kw_df, keyword=kw_en)\n",
    "    near_p, radius, exponent = params['near_p'], params['radius'], params['exponent']\n",
    "    p = models.core_periphery_model(distance, near_p[kw_en], radius[kw_en], exponent[kw_en])\n",
    "    model_all, model_word = \\\n",
    "        models.random_data_from_model(data_all, p, rng, num_repeat=5)\n",
    "    plot_all_vs_keyword(ax, model_all, model_word, \n",
    "                        style='contour', cmap='Reds', linecolor='#D63230')\n",
    "    ax.minorticks_off()\n",
    "\n",
    "def plot_all_vs_many_keywords(axes, all_kw_df, list_of_keywords, \n",
    "                              model='none', rng=None, with_jp=True, **params):\n",
    "    for i, keywords in enumerate(list_of_keywords):\n",
    "        for j, (keyword_jp, keyword_en) in enumerate(keywords):\n",
    "            print('Processing:', keyword_en)\n",
    "            if axes.ndim == 1:\n",
    "                ax = axes[j]\n",
    "            else:\n",
    "                ax = axes[i, j]\n",
    "            if model == 'none':\n",
    "                scatter_plot_all_vs_keyword_empirical(ax, all_kw_df, keyword_en)\n",
    "            elif model == 'binomial':\n",
    "                contour_plot_all_vs_keyword_binomial(ax, all_kw_df, keyword_en, rng, **params)\n",
    "            elif model == 'coreperi':\n",
    "                contour_plot_all_vs_keyword_coreperi(ax, all_kw_df, keyword_en, rng, **params)\n",
    "            ax.text(0.07, 0.95, keyword_en, ha='left', va='top', \n",
    "                    transform=ax.transAxes, fontsize=LARGE_FONT_SIZE)\n",
    "            if with_jp:\n",
    "                ax.text(0.07, 0.83, unicodedata.normalize('NFKC', keyword_jp), ha='left', va='top', \n",
    "                        transform=ax.transAxes, fontsize=MEDIUM_FONT_SIZE)\n",
    "            \n",
    "            if axes.ndim == 1 or i == len(axes) - 1:\n",
    "                ax.set_xlabel(r'$n_\\mathrm{all}$', labelpad=1)\n",
    "            else:\n",
    "                ax.set_xticklabels([])\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(r'$n_w$', labelpad=1)\n",
    "            else:\n",
    "                ax.set_yticklabels([])\n",
    "    return axes\n",
    "\n",
    "def plot_all_vs_many_keywords2(fig, all_kw_df, dict_of_keywords, \n",
    "                               model='none', rng=None, with_jp=True, **params):\n",
    "    subfigs = fig.subfigures(len(dict_of_keywords), 1, hspace=0.07, wspace=0)\n",
    "    axes = []\n",
    "    for i, (category, keywords) in enumerate(dict_of_keywords.items()):\n",
    "        subfig = subfigs[i]\n",
    "        subfig.text(-0.015, 0.5, category, ha='center', va='center', fontsize=LARGE_FONT_SIZE, rotation=90)\n",
    "        axes.append(subfig.subplots(1, len(keywords)))\n",
    "        for j, (keyword_jp, keyword_en) in enumerate(keywords):\n",
    "            print('Processing', keyword_en)\n",
    "            ax = axes[i][j]\n",
    "            if model == 'none':\n",
    "                scatter_plot_all_vs_keyword_empirical(ax, all_kw_df, keyword_en)\n",
    "            elif model == 'binomial':\n",
    "                contour_plot_all_vs_keyword_binomial(ax, all_kw_df, keyword_en, rng, **params)\n",
    "            elif model == 'coreperi':\n",
    "                contour_plot_all_vs_keyword_coreperi(ax, all_kw_df, keyword_en, rng, **params)\n",
    "            ax.text(0.07, 0.95, keyword_en, ha='left', va='top', \n",
    "                    transform=ax.transAxes, fontsize=LARGE_FONT_SIZE)\n",
    "            if with_jp:\n",
    "                ax.text(0.07, 0.81, unicodedata.normalize('NFKC', keyword_jp), ha='left', va='top', \n",
    "                        transform=ax.transAxes, fontsize=MEDIUM_FONT_SIZE)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=SMALL_FONT_SIZE, pad=0.5)\n",
    "            ax.set_xlabel(r'$n_\\mathrm{all}$', labelpad=-0.4)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(r'$n_w$', labelpad=0.8)\n",
    "            else:\n",
    "                ax.set_yticklabels([])\n",
    "    return subfigs, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names = [('東北', 'Tohoku'), ('関東', 'Kanto'), ('東海','Tokai'), \n",
    "                ('関西', 'Kansai'), ('四国', 'Shikoku'), ('九州', 'Kyushu')]\n",
    "pref_names = [('愛知', 'Aichi'), ('群馬', 'Gunma'), ('石川', 'Ishikawa'), \n",
    "              ('香川', 'Kagawa'), ('滋賀', 'Shiga'), ('宮城', 'Miyagi')]\n",
    "city_names = [('京都', 'Kyoto'), ('名古屋', 'Nagoya'), ('福岡', 'Fukuoka'), \n",
    "              ('広島', 'Hiroshima'), ('仙台', 'Sendai'), ('札幌', 'Sapporo')]\n",
    "ward_names = [('渋谷', 'Shibuya'), ('新宿', 'Shinjuku'), ('博多', 'Hakata'), \n",
    "              ('大宮', 'Omiya'), ('天王寺', 'Tennoji'), ('世田谷', 'Setagaya')]\n",
    "nonsp_common_nouns = [('電話', 'telephone'), ('音楽', 'music'), ('野菜', 'vegetable'), \n",
    "                      ('社会', 'society'), ('財布', 'wallet'), ('理由', 'reason')]\n",
    "sp_common_nouns = [('旅行', 'trip'), ('公園', 'park'),  ('大学', 'university'), \n",
    "                   ('ホテル', 'hotel'), ('空港', 'airport'), ('神社', 'shrine')]\n",
    "foreign_toponyms = [('ハワイ', 'Hawaii'), ('ドバイ', 'Dubai'), ('ホンコン/香港', 'Hong Kong'), \n",
    "                    ('ニューヨーク', 'New York'), ('シンガポール', 'Singapore'),  ('ソウル', 'Seoul')]\n",
    "list_of_keywords = region_names + pref_names + city_names + ward_names + \\\n",
    "    nonsp_common_nouns + sp_common_nouns + foreign_toponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "all_df = pd.read_csv(all_file, dtype={'m3code':str})\n",
    "if not all_df['m3code'].is_unique:\n",
    "    print('Total data: \\\"m3code\\\" is not unique.')\n",
    "all_df[['latitude', 'longitude']] = m3code.latlon(all_df)\n",
    "\n",
    "all_keyword_df = all_df.rename(columns={'tweetcount': 'tweetcount_all'})\n",
    "for kw_jp, kw_en in list_of_keywords:\n",
    "    kw_df = pd.read_csv(data_dir / file_names_dict[kw_en]['filename'], dtype={'m3code':str})\n",
    "    if not kw_df['m3code'].is_unique:\n",
    "        print(f'{kw_en} data: \\\"m3code\\\" is not unique.')\n",
    "    all_keyword_df = pd.merge(all_keyword_df, kw_df[['m3code', 'tweetcount']], on='m3code', how='left')\\\n",
    "        .rename(columns={'tweetcount': 'tweetcount_' + kw_en})\\\n",
    "        .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(12, 2.5))\n",
    "toponym_colors = plt.colormaps['Set2'](np.arange(6))\n",
    "toponym_symbols = ['o', 's', 'v', '^', 'd', 'x']\n",
    "for i, names in enumerate([region_names, pref_names, city_names, ward_names]):\n",
    "    ax = axes[i]\n",
    "    for j, (toponym_jp, toponym_en) in enumerate(names):\n",
    "        densities = all_keyword_df['tweetcount_'+toponym_en].to_numpy() / all_keyword_df['area'].to_numpy()\n",
    "        analysis.plot_single_histogram(\n",
    "            ax, densities, numbins=18, xscale='log', yscale='log', discrete=False, \n",
    "            marker=toponym_symbols[j], ms=4, ls='', color=toponym_colors[j], mfc='none', mew=1, label=toponym_en)\n",
    "    ax.minorticks_off()\n",
    "    ax.tick_params(axis='both', which='major', labelsize=SMALL_FONT_SIZE, pad=1)\n",
    "    ax.set_xlabel(r'$\\sigma_w$', labelpad=0.5)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$P(\\sigma_w)$', labelpad=1.5)\n",
    "    ax.legend(loc='upper right', fontsize=SMALL_FONT_SIZE, \n",
    "              handlelength=0.9, handletextpad=0.5, borderaxespad=0.4, labelspacing=0.4)\n",
    "    ax.set_xlim(left=0.5, right=3e5)\n",
    "    ax.set_ylim(bottom=5e-10, top=4)\n",
    "    ax.text(0.05, 0.05, r'$\\bf{'+chr(65+i)+'}$', transform=ax.transAxes, fontsize=PANEL_LABEL_SIZE, \n",
    "            font=PANEL_LABEL_FONT, color='black')\n",
    "\n",
    "fig.subplots_adjust(left=0.05, right=0.99, top=0.98, bottom=0.15)\n",
    "# fig.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='k', fill=False, lw=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'density_pdf_all_toponyms.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_keywords = {'Regions': region_names, 'Prefectures': pref_names, \n",
    "                    'Cities': city_names, 'Wards': ward_names, \n",
    "                    'Common nouns 1': nonsp_common_nouns,\n",
    "                    'Common nouns 2': sp_common_nouns,\n",
    "                    'Foreign places': foreign_toponyms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(dict_of_keywords)\n",
    "fig = plt.figure(figsize=(12, 2.2*nrows), layout='constrained')\n",
    "subfigs, axes = plot_all_vs_many_keywords2(fig, all_keyword_df, dict_of_keywords, \n",
    "                                           model='none', with_jp=True)\n",
    "\n",
    "bgcolors = plt.colormaps['Dark2'](np.array([0] * 4 + [1] * 2 + [2]))\n",
    "for subfig, bgcolor in zip(subfigs, bgcolors):\n",
    "    subfig.patch.set(facecolor=bgcolor, edgecolor='lightgrey', linewidth=1, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'all_tweets_vs_toponym_subsample_many.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location-independent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_binomial = {}\n",
    "aic_binomial = {}\n",
    "bic_binomial = {}\n",
    "for kws in dict_of_keywords.values():\n",
    "    for _, keyword_en in kws:\n",
    "        data_all = all_keyword_df['tweetcount_all'].to_numpy()\n",
    "        data_word = all_keyword_df['tweetcount_' + keyword_en].to_numpy()\n",
    "        p, aic, bic = models.binomial_mle(data_word, data_all)\n",
    "        p_binomial[keyword_en] = p\n",
    "        aic_binomial[keyword_en] = aic\n",
    "        bic_binomial[keyword_en] = bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell takes a few minutes\n",
    "nrows = len(dict_of_keywords)\n",
    "fig = plt.figure(figsize=(12, 2.2*nrows), layout='constrained')\n",
    "subfigs, axes = plot_all_vs_many_keywords2(fig, all_keyword_df, dict_of_keywords, \n",
    "                                           model='binomial', rng=rng, p_binomial=p_binomial)\n",
    "bgcolors = plt.colormaps['Dark2'](np.array([0] * 4 + [1] * 2 + [2]))\n",
    "for subfig, bgcolor in zip(subfigs, bgcolors):\n",
    "    subfig.patch.set(facecolor=bgcolor, edgecolor='lightgrey', linewidth=1, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'all_tweets_vs_toponym_subsample_many_binomial.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_empirical(sequence):\n",
    "    counts = np.bincount(sequence)\n",
    "    nonzero_counts = counts[counts > 0]\n",
    "    probs = nonzero_counts / len(sequence)\n",
    "    return -np.sum(probs * np.log(probs))\n",
    "\n",
    "def entropy_binomial(sequence, n, p):\n",
    "    likelihoods = scipy.stats.binom.pmf(sequence, n, p)\n",
    "    likelihoods = np.where(likelihoods > 1e-20, likelihoods, 1e-20)\n",
    "    return -np.sum(np.log(likelihoods)) / len(sequence)\n",
    "\n",
    "# def KL_divergence_from_binomial(sequence, n, p):\n",
    "#     return empirical_entropy_binomial(sequence, n, p) - empirical_entropy_empirical(sequence)\n",
    "\n",
    "def KL_divergence_from_binomial(nums_word, nums_all):\n",
    "    p = np.sum(nums_word) / np.sum(nums_all)\n",
    "    kld = 0\n",
    "    for n_all, counts in zip(*np.unique(nums_all, return_counts=True)):\n",
    "        sequence = nums_word[nums_all == n_all].astype(int)\n",
    "        kld += (entropy_binomial(sequence, n_all, p) - entropy_empirical(sequence)) * counts\n",
    "    return kld / len(nums_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell takes a few minutes\n",
    "relative_entropy = {kw_en: [] for _, kw_en in list_of_keywords}\n",
    "relative_entropy_nonzero = {kw_en: [] for _, kw_en in list_of_keywords}\n",
    "for _, kw_en in list_of_keywords:\n",
    "    print('Processing', kw_en)\n",
    "    all_onekw_df = all_keyword_df[['tweetcount_all', 'tweetcount_'+kw_en]]\n",
    "    all_onekw_nonzero_df = all_onekw_df[all_onekw_df['tweetcount_'+kw_en] > 0]\n",
    "    for _ in range(50):\n",
    "        nsample = 20000\n",
    "        sample_df = all_onekw_df.sample(n=nsample, replace=True, random_state=rng)\n",
    "        kld = KL_divergence_from_binomial(sample_df['tweetcount_'+kw_en].to_numpy(), \n",
    "                                          sample_df['tweetcount_all'].to_numpy())\n",
    "        relative_entropy[kw_en].append(kld)\n",
    "\n",
    "        nsample_nonzero = 3000\n",
    "        sample_nonzero_df = all_onekw_nonzero_df.sample(n=nsample_nonzero, replace=True, random_state=rng)\n",
    "        kld_nonzero = KL_divergence_from_binomial(sample_nonzero_df['tweetcount_'+kw_en].to_numpy(), \n",
    "                                                  sample_nonzero_df['tweetcount_all'].to_numpy())\n",
    "        relative_entropy_nonzero[kw_en].append(kld_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jungle_green = mcolors.to_rgba_array('#19906C')\n",
    "tea_green = mcolors.to_rgba_array('#D0F0C0')\n",
    "green_shades = np.array([tea_green + (jungle_green - tea_green) * i / 3 for i in range(4)])\n",
    "category_colors = [mcolors.to_hex(c) for c in green_shades]\n",
    "print(category_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [kw_en for _, kw_en in list_of_keywords]\n",
    "klds = [relative_entropy_nonzero[label] for label in labels]\n",
    "# colors = plt.colormaps['Dark2'](np.array([0] * 24 + [1] * 12 + [2] * 6))\n",
    "unique_colors = category_colors + ['#e89f67', '#d95f02', '#7570b3']\n",
    "colors = [c for c in unique_colors for _ in range(6)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "mean = np.mean(np.array(klds), axis=1)\n",
    "ci_low, ci_high = scipy.stats.bootstrap((np.array(klds).T, ), np.mean,\n",
    "                                        n_resamples=1000, confidence_level=0.95,\n",
    "                                        random_state=rng).confidence_interval\n",
    "xs = np.arange(1, len(labels) + 1)\n",
    "ax.bar(xs, mean, color=colors, zorder=5, edgecolor='k', lw=0.2, width=0.65)\n",
    "ax.vlines(xs, ci_low, ci_high, color='k', lw=1.2, zorder=5.1)\n",
    "yticks = np.arange(0, np.max(ci_high)*1.05, 4)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_ylim(bottom=0)\n",
    "ylabel = r'$D_\\mathrm{KL}(\\tilde{Q} \\parallel P)$'\n",
    "ax.set_ylabel(ylabel, fontsize=LARGE_FONT_SIZE)\n",
    "ax.set_xticks(xs, labels=labels, rotation=90)\n",
    "ax.set_xlim(0, len(labels)+1)\n",
    "ax.tick_params(axis='x', which='major', length=0)\n",
    "ax.grid(lw=0.5, axis='y')\n",
    "legend_handles = [mlines.Line2D([], [], color='white', marker='o',\n",
    "                                mfc=unique_colors[i], mec='none', ms=6) for i in range(7)]\n",
    "legend_labels = ['region', 'prefecture', 'city', 'ward', 'non-place noun', 'place noun', 'foreign toponym']\n",
    "ax.legend(legend_handles, legend_labels, loc='upper left', ncols=2, \n",
    "          handlelength=1.5, handletextpad=0.5)\n",
    "\n",
    "ax.set_position([0.06, 0.25, 0.93, 0.72])\n",
    "# fig.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='k', fill=False, lw=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'nonzero_relative_entropy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = {}\n",
    "for _, keyword_en in list_of_keywords:\n",
    "    total_count = all_keyword_df['tweetcount_' + keyword_en].sum()\n",
    "    total_counts[keyword_en] = total_count\n",
    "print('Minimum total count:', min(total_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [kw_en for _, kw_en in list_of_keywords]\n",
    "# colors = plt.colormaps['Dark2'](np.array([0] * 24 + [1] * 12 + [2] * 6))\n",
    "unique_colors = category_colors + ['#e89f67', '#d95f02', '#7570b3']\n",
    "colors = [c for c in unique_colors for _ in range(6)]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 7.5))\n",
    "ax_dict = fig.subplot_mosaic(\n",
    "        \"\"\"\n",
    "        AA\n",
    "        BC\n",
    "        \"\"\", \n",
    "        height_ratios=[1, 0.9],\n",
    "        gridspec_kw={'wspace': 0.25, 'hspace': 0.4}\n",
    "        )\n",
    "\n",
    "ax = ax_dict['A']\n",
    "klds = [relative_entropy[label] for label in labels]\n",
    "mean = np.mean(np.array(klds), axis=1)\n",
    "ci_low, ci_high = scipy.stats.bootstrap((np.array(klds).T, ), np.mean,\n",
    "                                        n_resamples=1000, confidence_level=0.95,\n",
    "                                        random_state=rng).confidence_interval\n",
    "xs = np.arange(1, len(labels) + 1)\n",
    "\n",
    "ax.bar(xs, mean, color=colors, zorder=5, edgecolor='k', lw=0.2, width=0.65, alpha=0.95)\n",
    "ax.vlines(xs, ci_low, ci_high, color='k', lw=1.2, zorder=5.1)\n",
    "yticks = np.arange(0, np.max(ci_high)*1.05, 0.5)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_ylim(bottom=0)\n",
    "ylabel = r'$D_\\mathrm{KL}(Q \\parallel P)$'\n",
    "ax.set_ylabel(ylabel, fontsize=LARGE_FONT_SIZE)\n",
    "ax.set_xticks(xs, labels=labels, rotation=90)\n",
    "ax.set_xlim(0, len(labels)+1)\n",
    "ax.tick_params(axis='x', which='major', length=0)\n",
    "ax.grid(lw=0.5, axis='y')\n",
    "legend_handles = [mlines.Line2D([], [], color='white', marker='o',\n",
    "                                mfc=unique_colors[i], mec='none', ms=6) for i in range(7)]\n",
    "legend_labels = ['region', 'prefecture', 'city', 'ward', 'non-place noun', 'place noun', 'foreign toponym']\n",
    "ax.legend(legend_handles, legend_labels, loc='upper left', ncols=2, \n",
    "          handlelength=1.5, handletextpad=0.5)\n",
    "ax.text(-0.05, 0.97, 'A', ha='left', va='bottom', weight='bold',\n",
    "        transform=ax.transAxes, font=PANEL_LABEL_FONT, fontsize=PANEL_LABEL_SIZE)\n",
    "\n",
    "total_count_arr = [total_counts[label] for label in labels]\n",
    "\n",
    "for i, rel_ent in enumerate([relative_entropy, relative_entropy_nonzero]):\n",
    "    panel_label = 'B' if i == 0 else 'C'\n",
    "    ylabel = r'$D_\\mathrm{KL}(Q \\parallel P)$' if i == 0 else r'$D_\\mathrm{KL}(\\tilde{Q} \\parallel P)$'\n",
    "    ytick_interval = 0.5 if i == 0 else 4\n",
    "\n",
    "    ax = ax_dict[panel_label]\n",
    "    klds = [rel_ent[label] for label in labels]\n",
    "    mean = np.mean(np.array(klds), axis=1)\n",
    "    ax.plot(total_count_arr, mean, 'o', ms=5, color='k', mec='k', mew=1, alpha=0.5)\n",
    "    # Calculate correlation coefficient\n",
    "    r, p = scipy.stats.pearsonr(total_count_arr, mean)\n",
    "    ax.text(0.97, 0.03, f'$r = {r:.2f}$', ha='right', va='bottom', \n",
    "            transform=ax.transAxes, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.set_xlabel(r'$N_w$', labelpad=2, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.set_yticks(np.arange(0, np.max(mean)*1.05, ytick_interval))\n",
    "    ax.set_ylabel(ylabel, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.text(-0.11, 0.97, panel_label, ha='left', va='bottom', weight='bold', \n",
    "            transform=ax.transAxes, font=PANEL_LABEL_FONT, fontsize=PANEL_LABEL_SIZE)\n",
    "\n",
    "fig.subplots_adjust(left=0.06, right=0.99, top=0.97, bottom=0.07)\n",
    "# fig.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='k', fill=False, lw=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'relative_entropy_vs_total_count.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "labels, klds = zip(*relative_entropy.items())\n",
    "klds_nonzero = [relative_entropy_nonzero[label] for label in labels]\n",
    "total_count_arr = [total_counts[label] for label in labels]\n",
    "colors = plt.colormaps['Dark2'](np.array([0] * 24 + [1] * 12 + [2] * 6))\n",
    "\n",
    "fig, axes = plt.subplots(2, ncols, figsize=(4 * ncols, 14), \n",
    "                         gridspec_kw={'height_ratios': [4.5, 1], 'wspace': 0.35, 'hspace': 0.15})\n",
    "panel_labels = np.array(list(map(chr, range(ord('A'), ord('A') + 2 * ncols)))).reshape(2, ncols)\n",
    "for col in range(ncols):\n",
    "    if col == 0:\n",
    "        mean = np.mean(np.array(klds), axis=1)\n",
    "        ci_low, ci_high = scipy.stats.bootstrap((np.array(klds).T, ), np.mean, \n",
    "                                                n_resamples=1000, confidence_level=0.95, \n",
    "                                                random_state=rng).confidence_interval\n",
    "        xlabel = r'$D_\\mathrm{KL}(Q \\parallel P)$'\n",
    "        xticks = None\n",
    "    else:\n",
    "        mean = np.mean(np.array(klds_nonzero), axis=1)\n",
    "        ci_low, ci_high = scipy.stats.bootstrap((np.array(klds_nonzero).T, ), np.mean,\n",
    "                                                n_resamples=1000, confidence_level=0.95,\n",
    "                                                random_state=rng).confidence_interval\n",
    "        xlabel = r'$D_\\mathrm{KL}(\\tilde{Q} \\parallel P)$'\n",
    "        xticks = np.arange(0, np.max(ci_high)*1.05, 4)\n",
    "    ax = axes[0, col]\n",
    "    ys = np.arange(len(labels), 0, -1)\n",
    "    ax.scatter(mean, ys, color=colors, s=20, zorder=5)\n",
    "    ax.hlines(ys, ci_low, ci_high, color=colors, lw=1.5, zorder=4.9)\n",
    "    # ax.axvline(0, color='darkgrey', lw=2, zorder=2.1)\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_yticks(ys, labels=labels)\n",
    "    ax.grid(lw=0.5)\n",
    "    ax.set_ylim(0, len(labels)+1)\n",
    "    ax.set_xlabel(xlabel, labelpad=1)\n",
    "    ax.text(-0.1, 1, panel_labels[0][col],\n",
    "            ha='left', va='bottom', weight='bold', \n",
    "            transform=ax.transAxes, font=PANEL_LABEL_FONT, fontsize=PANEL_LABEL_SIZE)\n",
    "    if col == 0:\n",
    "        unique_colors = colors[[0, 24, 36]]\n",
    "        ax.legend([mlines.Line2D([], [], color='white', marker='o', \n",
    "                                 mfc=unique_colors[i], mec='none', ms=6) for i in range(3)],\n",
    "                   ['domestic toponym', 'common noun', 'foreign toponym'], loc='upper right')\n",
    "    if xticks is not None:\n",
    "        ax.set_xticks(xticks)\n",
    "\n",
    "    ax = axes[1, col]\n",
    "    ax.plot(total_count_arr, mean, \n",
    "            'o', ms=4, color='k', mec='k', mew=1, alpha=0.5)\n",
    "    # Calculate correlation coefficient\n",
    "    r, p = scipy.stats.pearsonr(total_count_arr, mean)\n",
    "    ax.text(0.97, 0.03, f'$r = {r:.2f}$',\n",
    "            ha='right', va='bottom', transform=ax.transAxes, fontsize=SMALL_FONT_SIZE)\n",
    "    ax.set_xlabel(r'$N_w$', labelpad=1.5)\n",
    "    ax.set_ylabel(xlabel)\n",
    "    ax.text(-0.1, 1.02, panel_labels[1][col],\n",
    "            ha='left', va='bottom', weight='bold', \n",
    "            transform=ax.transAxes, font=PANEL_LABEL_FONT, fontsize=PANEL_LABEL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core periphery model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toponym_categories = ['Regions', 'Prefectures', 'Cities', 'Wards']\n",
    "dict_of_toponyms = {category: dict_of_keywords[category] for category in toponym_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_periphery_mle_std(num_target, num_all, dist, params):\n",
    "    # Compute standard error through Fisher information matrix\n",
    "    q, r, a = params\n",
    "    p = models.core_periphery_model(dist, q, r, a)\n",
    "    dpdr = np.where(dist < r, 0, q * (a / r) * (dist / r)**(-a))\n",
    "    dpdrr = np.where(dist < r, 0, q * (a * (a - 1) / r**2) * (dist / r)**(-a))\n",
    "    # dpda = np.where(dist < r, 0, - q * np.log(dist / r) * (dist / r)**(-a))\n",
    "    # dpdaa = np.where(dist < r, 0, q * np.log(dist / r)**2 * (dist / r)**(-a))\n",
    "    dLdp = (num_target / p + (num_all - num_target) / (1 - p))\n",
    "    dLdpp = -(num_target / p**2 + (num_all - num_target) / (1 - p)**2)\n",
    "    dLdrr = dLdpp * dpdr**2 + dLdp * dpdrr\n",
    "    # dLdaa = dLdpp * dpda**2 + dLdp * dpdaa\n",
    "    return 1 / np.sqrt(- np.sum(dLdrr)) # , 1 / np.sqrt(- np.sum(dLdaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_p = {}\n",
    "radius = {}\n",
    "exponent = {}\n",
    "aic_coreperi = {}\n",
    "bic_coreperi = {}\n",
    "\n",
    "maximum_radius_std = 0\n",
    "\n",
    "for kws in dict_of_toponyms.values():\n",
    "    for _, keyword_en in kws:\n",
    "        print('Processing', keyword_en)\n",
    "        data_all = all_keyword_df['tweetcount_all'].to_numpy()\n",
    "        data_word = all_keyword_df['tweetcount_' + keyword_en].to_numpy()\n",
    "        distance = analysis.distance_from_center(all_keyword_df, keyword=keyword_en)\n",
    "        params, aic, bic = \\\n",
    "            models.core_periphery_mle(data_word, data_all, distance)\n",
    "        near_p[keyword_en] = params[0]\n",
    "        radius[keyword_en] = params[1]\n",
    "        exponent[keyword_en] = params[2]\n",
    "        aic_coreperi[keyword_en] = aic\n",
    "        bic_coreperi[keyword_en] = bic\n",
    "        radius_std = core_periphery_mle_std(data_word, data_all, distance, params)\n",
    "        maximum_radius_std = max(maximum_radius_std, radius_std)\n",
    "        print(f'{keyword_en:<12} : near_p = {near_p[keyword_en]:.4f}, '\n",
    "              f'radius = {radius[keyword_en]:<6.3g}, '\n",
    "              f'radius std = {radius_std:.3g}, ', \n",
    "              f'exponent = {exponent[keyword_en]:.3f}')\n",
    "        \n",
    "print('Maximum radius std:', maximum_radius_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(dict_of_toponyms)\n",
    "fig = plt.figure(figsize=(12, 2.2*nrows), layout='constrained')\n",
    "subfigs, axes = plot_all_vs_many_keywords2(fig, all_keyword_df, dict_of_toponyms,\n",
    "                           model='coreperi', rng=rng, near_p=near_p, radius=radius, exponent=exponent)\n",
    "bgcolors = plt.colormaps['Dark2'](np.array([0] * 4))\n",
    "for subfig, bgcolor in zip(subfigs, bgcolors):\n",
    "    subfig.patch.set(facecolor=bgcolor, edgecolor='lightgrey', linewidth=1, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'all_tweets_vs_toponym_subsample_many_coreperi.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_toponym = 'Fukuoka'\n",
    "lim_color = '#89b0ae'\n",
    "cpm_color = '#555b6e'\n",
    "tick_param_kwargs = {'axis': 'both', 'which': 'major', 'labelsize': MEDIUM_FONT_SIZE, 'pad': 2}\n",
    "\n",
    "for with_model in [False, True]:\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    data_all = all_keyword_df['tweetcount_all'].to_numpy()\n",
    "    data_word = all_keyword_df['tweetcount_' + big_toponym].to_numpy()\n",
    "    distance = analysis.distance_from_center(all_keyword_df, keyword=big_toponym)\n",
    "    ratio = data_word / data_all\n",
    "\n",
    "    logbins = np.geomspace(0.5, np.max(distance), 30)\n",
    "    logbinned_distance = scipy.stats.binned_statistic(distance, distance, statistic='mean', bins=logbins).statistic\n",
    "    logbinned_ratio = scipy.stats.binned_statistic(distance, ratio, statistic='mean', bins=logbins).statistic\n",
    "    ax.plot(distance, ratio, marker='o', ms=1.2, ls='', color='k', \n",
    "            alpha=0.2, mec='none', rasterized=True)\n",
    "    ax.plot(logbinned_distance, logbinned_ratio, marker='o', ms=4.5, ls='', lw=2, \n",
    "            alpha=0.85, color='#e63946', zorder=2.5, label='Binned average')\n",
    "\n",
    "    p_binom, aic_binom, bic_binom = models.binomial_mle(data_word, data_all)\n",
    "    x_model = np.geomspace(1, np.max(distance), 200)\n",
    "    y_model = models.core_periphery_model(x_model, *params)\n",
    "    if with_model:\n",
    "        ax.hlines(p_binom, 1, np.max(distance), lw=2, color=lim_color, ls='-', \n",
    "                  label='Location-independent model', alpha=0.8)\n",
    "        ax.plot(x_model, y_model, lw=2, color=cpm_color, ls='--', \n",
    "                label='Core-periphery model', alpha=0.9)\n",
    "    ax.legend(loc='lower left', fontsize=MEDIUM_FONT_SIZE)\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel(r'$\\phi_w$', labelpad=1.2, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.tick_params(**tick_param_kwargs)\n",
    "    ax.set_xlim(left=0.7, right=3E3)\n",
    "    ax.set_xlabel('distance from center ' + r'$d_w$ ' + '(km)', labelpad=0.9, fontsize=LARGE_FONT_SIZE)\n",
    "\n",
    "    fig.savefig(figdir / f'occurrence_ratio_vs_distance_{big_toponym}_{\"with_model\" if with_model else \"empirical\"}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize core-periphery model in one figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For map visualization\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import mapviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dms2dd(dms):\n",
    "    '''Convert the degrees, minutes, seconds notation to decimal degrees'''\n",
    "    return dms[0] + dms[1] / 60 + dms[2] / 3600\n",
    "\n",
    "# Reachable extreme points of Japan\n",
    "east_ext = dms2dd((145, 48, 58))\n",
    "west_ext = dms2dd((122, 56, 1))\n",
    "south_ext = dms2dd((24, 2, 59))\n",
    "north_ext = dms2dd((45, 31, 22))\n",
    "\n",
    "center_lon = dms2dd((139, 44, 28))\n",
    "center_lat = dms2dd((35, 39, 29))\n",
    "lon_diff = east_ext - west_ext\n",
    "lat_diff = north_ext - south_ext\n",
    "extent = (west_ext-lon_diff*0.05, east_ext+lon_diff*0.05, \n",
    "          south_ext-lat_diff*0.05, north_ext+lat_diff*0.05)\n",
    "\n",
    "lambert_area_proj = ccrs.LambertAzimuthalEqualArea(\n",
    "    central_longitude=center_lon, \n",
    "    central_latitude=center_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shapefiles of prefectures\n",
    "shpfile_pref = shpreader.natural_earth(resolution='10m', category='cultural', \n",
    "                                       name='admin_1_states_provinces')\n",
    "pref_shape_dict = {pref.attributes['name'].translate(str.maketrans({'Ō': 'O', 'ō' : 'o'})): pref.geometry \n",
    "                   for pref in shpreader.Reader(shpfile_pref).records()\n",
    "                   if pref.attributes['admin'] == 'Japan'}\n",
    "\n",
    "# Get shapefiles of metropolitan employment areas (MEAs)\n",
    "mea_dir = Path().resolve().parent / 'original' / 'japan_MetropolitanEmploymentArea2015map'\n",
    "shpfile_mea = mea_dir / 'japan_MetropolitanEmploymentArea2015map.shp'\n",
    "mea_shape_dict = {mea.attributes['MEA_Name']: mea.geometry \n",
    "                  for mea in shpreader.Reader(shpfile_mea).records()}\n",
    "print('Number of MEAs:', len(mea_shape_dict))\n",
    "\n",
    "# The city name and MEA name is not exactly the same.\n",
    "# Make a dictionary to convert city names to MEA names.\n",
    "city_name_to_mea_name = {}\n",
    "for city_jp, city_en in city_names:\n",
    "    for mea_name in mea_shape_dict.keys():\n",
    "        if city_en in mea_name:\n",
    "            print(city_en, city_jp, mea_name)\n",
    "            city_name_to_mea_name[city_en] = mea_name\n",
    "city_shape_dict = {city_name: mea_shape_dict[mea_name] \n",
    "                   for city_name, mea_name in city_name_to_mea_name.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_label_kwargs = {'ha': 'left', 'va': 'bottom', 'weight': 'bold', \n",
    "                      'fontsize': PANEL_LABEL_SIZE, 'font': PANEL_LABEL_FONT}\n",
    "colors = [c for c in category_colors for _ in range(6)]\n",
    "lim_color = '#89b0ae'\n",
    "cpm_color = '#555b6e'\n",
    "toponym_colors = plt.colormaps['Set2'](np.arange(6))\n",
    "big_toponym = 'Fukuoka'\n",
    "tick_param_kwargs = {'axis': 'both', 'which': 'major', 'labelsize': MEDIUM_FONT_SIZE, 'pad': 2}\n",
    "scale_bar_loc = (0.8, 0.05)\n",
    "scale_bar_kwargs = dict(color='dimgrey', text_kwargs={'size': SMALL_FONT_SIZE})\n",
    "\n",
    "fig = plt.figure(figsize=(12, 14))\n",
    "subfig_ABC, subfig_D = fig.subfigures(2, 1, height_ratios=[1, 0.43])\n",
    "subfig_AC, subfig_B = subfig_ABC.subfigures(1, 2, width_ratios=[1.2, 1], wspace=0.03)\n",
    "subfig_A, subfig_C = subfig_AC.subfigures(2, 1, height_ratios=[1, 1.4])\n",
    "\n",
    "big_toponym_ax = subfig_A.add_subplot()\n",
    "axes_B = subfig_B.subplots(4, 1)\n",
    "ax_rad = subfig_C.add_subplot()\n",
    "axes_D = subfig_D.subplots(2, 6, subplot_kw=dict(projection=lambert_area_proj), \n",
    "                           gridspec_kw={'wspace': 0.1, 'hspace': 0.1})\n",
    "\n",
    "# Panel A/B\n",
    "for i, category in enumerate(toponym_categories):\n",
    "    names = dict_of_toponyms[category]\n",
    "    ax = axes_B[i]\n",
    "    ax.text(0.03, 0.04, category, transform=ax.transAxes, \n",
    "            ha='left', va='bottom', fontsize=LARGE_FONT_SIZE)\n",
    "    for j, (toponym, toponym_en) in enumerate(names):\n",
    "        print('Processing', toponym_en)\n",
    "        data_all = all_keyword_df['tweetcount_all'].to_numpy()\n",
    "        data_word = all_keyword_df['tweetcount_' + toponym_en].to_numpy()\n",
    "        distance = analysis.distance_from_center(all_keyword_df, keyword=toponym_en)\n",
    "        ratio = data_word / data_all\n",
    "        \n",
    "        logbins = np.geomspace(0.5, np.max(distance), 30)\n",
    "        logbinned_distance = scipy.stats.binned_statistic(distance, distance, statistic='mean', bins=logbins).statistic\n",
    "        logbinned_ratio = scipy.stats.binned_statistic(distance, ratio, statistic='mean', bins=logbins).statistic\n",
    "        ax.plot(logbinned_distance, logbinned_ratio, '-', lw=2, alpha=0.9, color=toponym_colors[j], label=toponym_en)\n",
    "        \n",
    "        if toponym_en == big_toponym:\n",
    "            big_toponym_ax.plot(distance, ratio, marker='o', ms=1.2, ls='', color='k', \n",
    "                    alpha=0.2, mec='none', rasterized=True)\n",
    "            big_toponym_ax.plot(logbinned_distance, logbinned_ratio, marker='o', ms=4.5, ls='', lw=2, \n",
    "                    alpha=0.85, color='#e63946', zorder=2.5, label='Binned average')\n",
    "            \n",
    "            p_binom, aic_binom, bic_binom = models.binomial_mle(data_word, data_all)\n",
    "            x_model = np.geomspace(1, np.max(distance), 200)\n",
    "            y_model = models.core_periphery_model(x_model, near_p[big_toponym], radius[big_toponym], exponent[big_toponym])\n",
    "            big_toponym_ax.hlines(p_binom, 1, np.max(distance), lw=2, color=lim_color, ls='-', \n",
    "                                  label='Location-independent model', alpha=0.8)\n",
    "            big_toponym_ax.plot(x_model, y_model, lw=2, color=cpm_color, ls='--', \n",
    "                                label='Core-periphery model', alpha=0.9)\n",
    "            big_toponym_ax.legend(loc='lower left', fontsize=MEDIUM_FONT_SIZE)\n",
    "\n",
    "# Panel C\n",
    "flatten_toponyms = [toponym_en for names in dict_of_toponyms.values() for _, toponym_en in names]\n",
    "radius_arr = [radius[toponym_en] for toponym_en in flatten_toponyms]\n",
    "ax_rad.barh(np.arange(len(radius_arr))[::-1], radius_arr, color=colors, edgecolor='grey', zorder=1.6)\n",
    "ax_rad.set_yticks(np.arange(len(radius_arr))[::-1], flatten_toponyms)\n",
    "ax_rad.grid(axis='y', zorder=0.9)\n",
    "ax_rad.set_ylim(-1, len(radius_arr))\n",
    "ax.tick_params(axis='y', which='major', length=0)\n",
    "\n",
    "ax_exp = ax_rad.twiny()\n",
    "exponent_arr = [exponent[toponym_en] for toponym_en in flatten_toponyms]\n",
    "ax_exp.plot(exponent_arr, np.arange(len(exponent_arr))[::-1], 'o', \n",
    "            color='dimgrey', ms=5, mec='k', mew=1, alpha=0.7, zorder=1.1)\n",
    "\n",
    "# Panel D\n",
    "# shuffled_df = all_keyword_df.sample(frac=1, random_state=rng)\n",
    "for i, (names, shape_dict) in enumerate(zip([pref_names, city_names], [pref_shape_dict, city_shape_dict])):\n",
    "    for j, (_, toponym_en) in enumerate(names):\n",
    "        print('Processing', toponym_en)\n",
    "        print(f'Radius: {radius[toponym_en]:.2f} km, Exponent: {exponent[toponym_en]:.2f}')\n",
    "        nonzero_df = all_keyword_df[all_keyword_df['tweetcount_' + toponym_en] > 0].sample(frac=1, random_state=rng)\n",
    "        data_lon = nonzero_df['longitude'].to_numpy()\n",
    "        data_lat = nonzero_df['latitude'].to_numpy()\n",
    "        data_val = (nonzero_df['tweetcount_' + toponym_en] / nonzero_df['tweetcount_all']).to_numpy()\n",
    "        norm = mcolors.LogNorm(vmax=np.quantile(data_val, 0.99))\n",
    "        cmap = plt.get_cmap('Oranges', 8)\n",
    "        center_latlon = analysis.center(nonzero_df, keyword=toponym_en)\n",
    "        map_extent = (\n",
    "            center_latlon[1]-lon_diff*0.1, \n",
    "            center_latlon[1]+lon_diff*0.1, \n",
    "            center_latlon[0]-lat_diff*0.08, \n",
    "            center_latlon[0]+lat_diff*0.08)\n",
    "        ax = axes_D[i, j]\n",
    "        map_data = mapviz.MapVisualizer(fig, extent=map_extent, projection=lambert_area_proj, \n",
    "                                        color='grey', resolution='10m', ax=ax, lw=0.5)\n",
    "        map_data.visualize(data_lon, data_lat, data_val, cmap=cmap, norm=norm, \n",
    "                        size=0.4, marker=',', alpha=0.4, rasterized=True)\n",
    "        toponym_text = map_data.ax.text(0.04, 0.95, toponym_en, ha='left', va='top',\n",
    "                        transform=map_data.ax.transAxes, fontsize=MEDIUM_FONT_SIZE)\n",
    "        toponym_text.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        for factor, ec in zip([1, 2, 3], ['#2f6244', '#4ea472', '#8cc8a5']):\n",
    "            map_data.add_circle(center_latlon[1], center_latlon[0],\n",
    "                                radius[toponym_en] * 1000 * np.power(factor, 1 / exponent[toponym_en]),\n",
    "                                fc='none', ec=ec, lw=0.8, zorder=4)\n",
    "        shape = shape_dict[toponym_en]\n",
    "        shape_feature = cfeature.ShapelyFeature(shape, ccrs.PlateCarree(), \n",
    "                                                fc=mcolors.to_rgba('tab:purple', 0.5), \n",
    "                                                ec=mcolors.to_rgba('tab:purple', 0.8), lw=0.8)\n",
    "        map_data.ax.add_feature(shape_feature)\n",
    "        mapviz.scale_bar(ax=map_data.ax, location=scale_bar_loc, length=50, \n",
    "                         **scale_bar_kwargs)\n",
    "\n",
    "for ax in [big_toponym_ax] + axes_B.tolist():\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel(r'$\\phi_w$', labelpad=1.2, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.tick_params(**tick_param_kwargs)\n",
    "    ax.set_xlim(left=0.7, right=3E3)\n",
    "\n",
    "for ax in axes_B:\n",
    "    ax.legend(loc='upper left', fontsize=MEDIUM_FONT_SIZE, bbox_to_anchor=(1.01, 1), handlelength=1.5)\n",
    "\n",
    "for ax in [big_toponym_ax, axes_B[-1]]:\n",
    "    ax.set_xlabel('distance from center ' + r'$d_w$ ' + '(km)', labelpad=0.9, fontsize=LARGE_FONT_SIZE)\n",
    "\n",
    "ax_rad.set_xlabel('radius ' + r'$\\hat{r}_w$ ' + '(km)', fontsize=LARGE_FONT_SIZE)\n",
    "ax_rad.tick_params(**tick_param_kwargs)\n",
    "ax_exp.set_xlabel('exponent ' + r'$\\hat{a}_w$', fontsize=LARGE_FONT_SIZE, labelpad=6)\n",
    "ax_exp.tick_params(**tick_param_kwargs)\n",
    "\n",
    "# import matplotlib.patches as mpatches\n",
    "# fig.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='k', fill=False, lw=1))\n",
    "# subfig_A.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='b', fill=False, lw=1))\n",
    "# subfig_C.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='r', fill=False, lw=1))\n",
    "# subfig_B.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='g', fill=False, lw=1))\n",
    "# subfig_D.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='y', fill=False, lw=1))\n",
    "\n",
    "subfig_A.subplots_adjust(top=0.88, bottom=0.17, left=0.16, right=0.92)\n",
    "subfig_C.subplots_adjust(top=0.91, bottom=0.14, left=0.16, right=0.92)\n",
    "subfig_B.subplots_adjust(top=0.95, bottom=0.08, left=0.16, right=0.73)\n",
    "subfig_D.subplots_adjust(top=0.92, bottom=0.05, left=0.05, right=0.97)\n",
    "\n",
    "subfig_A.text(0.05, 0.91, 'A', **panel_label_kwargs)\n",
    "subfig_C.text(0.05, 0.95, 'C', **panel_label_kwargs)\n",
    "subfig_B.text(0.03, 0.96, 'B', **panel_label_kwargs)\n",
    "subfig_D.text(0.025, 0.94, 'D', **panel_label_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'core_periphery.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7.2, 3), layout='constrained')\n",
    "x = np.arange(len(aic_coreperi))\n",
    "width = 0.35\n",
    "multiplier = 0\n",
    "\n",
    "lim_color = '#89b0ae' # '#2a9d8f' # '#BCA3CA' # '#a8dadc'\n",
    "cpm_color = '#555b6e' # '#264653' # '#CCF0C3' # '#e63946'\n",
    "\n",
    "toponyms_en = [toponym_en for names in dict_of_toponyms.values() for _, toponym_en in names]\n",
    "ax.bar(x, [aic_binomial[toponym_en] for toponym_en in toponyms_en],\n",
    "       width, label='location-indepedent model', color=lim_color)\n",
    "ax.bar(x + width, [aic_coreperi[toponym_en] for toponym_en in toponyms_en],\n",
    "       width, label='core-periphery model', color=cpm_color)\n",
    "ax.set_xticks(x + width / 2, [toponym_en for toponym_en in toponyms_en], rotation=90)\n",
    "ax.tick_params(axis='x', which='major', length=0)\n",
    "ax.set_xlim(-1, len(toponyms_en))\n",
    "ax.set_ylabel('AIC', fontsize=LARGE_FONT_SIZE)\n",
    "ax.legend(fontsize=MEDIUM_FONT_SIZE, handlelength=1.5)\n",
    "# fig.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='k', fill=False, lw=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'aic_comparison.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots for representative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = ['Fukuoka', 'wallet', 'airport', 'Hawaii']\n",
    "example_word_coreperi = 'Fukuoka'\n",
    "\n",
    "panel_label_kwargs = {'ha': 'left', 'va': 'top', 'weight': 'bold',\n",
    "                      'fontsize': PANEL_LABEL_SIZE, 'font': PANEL_LABEL_FONT}\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4.8))\n",
    "delta = 0.38\n",
    "lr_A = 0.925\n",
    "lr_B = 0.72\n",
    "left_A = 0.058\n",
    "left_B = 0.221\n",
    "AB_width_ratio = (4 + 3 * delta) * lr_B / lr_A\n",
    "print(AB_width_ratio)\n",
    "\n",
    "subfigs_2x2 = fig.subfigures(2, 2, width_ratios=[AB_width_ratio, 1], wspace=0.04, hspace=0.09)\n",
    "subfigs = [subfigs_2x2[0, 0], subfigs_2x2[1, 0], subfigs_2x2[1, 1]]\n",
    "for subfig in subfigs:\n",
    "    subfig.add_artist(mpatches.Rectangle((0, 0), 1, 1, facecolor='lightgray', fill=True, \n",
    "                                         edgecolor='none', lw=1, zorder=0))\n",
    "\n",
    "axes_scatter = subfigs[0].subplots(1, len(example_words), gridspec_kw={'wspace': delta})\n",
    "axes_binomial = subfigs[1].subplots(1, len(example_words), gridspec_kw={'wspace': delta})\n",
    "ax_coreperi = subfigs[2].add_subplot()\n",
    "\n",
    "for subfig in [subfigs[0], subfigs[1]]:\n",
    "    subfig.subplots_adjust(left=left_A, right=left_A+lr_A, top=0.9, bottom=0.17)\n",
    "subfigs[2].subplots_adjust(left=left_B, right=left_B+lr_B, top=0.9, bottom=0.17)\n",
    "\n",
    "for subfig, panel_label in zip(subfigs, ['A', 'B', 'C']):\n",
    "    if panel_label == 'C':\n",
    "        subfig.text(0.008 * AB_width_ratio, 0.98, panel_label, **panel_label_kwargs)\n",
    "    else:\n",
    "        subfig.text(0.008, 0.98, panel_label, **panel_label_kwargs)\n",
    "\n",
    "for axes in [axes_scatter, axes_binomial]:\n",
    "    for ax, word_en in zip(axes, example_words):\n",
    "        ax.text(0.05, 0.95, word_en, ha='left', va='top',\n",
    "                transform=ax.transAxes, fontsize=LARGE_FONT_SIZE)\n",
    "\n",
    "ax_coreperi.text(0.05, 0.95, example_word_coreperi, ha='left', va='top', \n",
    "                 transform=ax_coreperi.transAxes, fontsize=LARGE_FONT_SIZE)\n",
    "\n",
    "for ax in axes_scatter.tolist() + axes_binomial.tolist() + [ax_coreperi]:\n",
    "    ax.set_xlabel(r'$n_\\mathrm{all}$', labelpad=1, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.set_ylabel(r'$n_w$', labelpad=0.8, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=SMALL_FONT_SIZE, pad=1)\n",
    "\n",
    "# fig.add_artist(mpatches.Rectangle((0, 0), 1, 1, color='k', fill=False, lw=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = ['Fukuoka', 'wallet', 'airport', 'Hawaii']\n",
    "example_word_coreperi = 'Fukuoka'\n",
    "\n",
    "panel_label_kwargs = {'ha': 'left', 'va': 'top', 'weight': 'bold',\n",
    "                      'fontsize': PANEL_LABEL_SIZE, 'font': PANEL_LABEL_FONT}\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4.8))\n",
    "delta = 0.38\n",
    "lr_A = 0.925\n",
    "lr_B = 0.72\n",
    "left_A = 0.058\n",
    "left_B = 0.221\n",
    "AB_width_ratio = (4 + 3 * delta) * lr_B / lr_A\n",
    "print(AB_width_ratio)\n",
    "\n",
    "subfigs_2x2 = fig.subfigures(2, 2, width_ratios=[AB_width_ratio, 1], wspace=0.04, hspace=0.09)\n",
    "subfigs = [subfigs_2x2[0, 0], subfigs_2x2[1, 0], subfigs_2x2[1, 1]]\n",
    "for subfig in subfigs:\n",
    "    subfig.add_artist(mpatches.Rectangle((0, 0), 1, 1, facecolor='lightgray', fill=True, \n",
    "                                         edgecolor='none', lw=1, zorder=0))\n",
    "\n",
    "axes_scatter = subfigs[0].subplots(1, len(example_words), gridspec_kw={'wspace': delta})\n",
    "axes_binomial = subfigs[1].subplots(1, len(example_words), gridspec_kw={'wspace': delta})\n",
    "ax_coreperi = subfigs[2].add_subplot()\n",
    "\n",
    "for subfig in [subfigs[0], subfigs[1]]:\n",
    "    subfig.subplots_adjust(left=left_A, right=left_A+lr_A, top=0.91, bottom=0.18)\n",
    "subfigs[2].subplots_adjust(left=left_B, right=left_B+lr_B, top=0.91, bottom=0.18)\n",
    "\n",
    "for subfig, panel_label in zip(subfigs, ['A', 'B', 'C']):\n",
    "    if panel_label == 'C':\n",
    "        subfig.text(0.008 * AB_width_ratio, 0.98, panel_label, **panel_label_kwargs)\n",
    "    else:\n",
    "        subfig.text(0.008, 0.98, panel_label, **panel_label_kwargs)\n",
    "\n",
    "for ax, kw in zip(axes_scatter, example_words):\n",
    "    ax.text(0.05, 0.95, kw, ha='left', va='top',\n",
    "            transform=ax.transAxes, fontsize=LARGE_FONT_SIZE)\n",
    "    scatter_plot_all_vs_keyword_empirical(ax, all_keyword_df, kw)\n",
    "    \n",
    "for ax, kw in zip(axes_binomial, example_words):\n",
    "    ax.text(0.05, 0.95, kw, ha='left', va='top',\n",
    "            transform=ax.transAxes, fontsize=LARGE_FONT_SIZE)\n",
    "    \n",
    "    contour_plot_all_vs_keyword_binomial(ax, all_keyword_df, kw, \n",
    "                                         rng=rng, p_binomial=p_binomial)\n",
    "\n",
    "ax_coreperi.text(0.05, 0.95, example_word_coreperi, ha='left', va='top', \n",
    "                 transform=ax_coreperi.transAxes, fontsize=LARGE_FONT_SIZE)\n",
    "contour_plot_all_vs_keyword_coreperi(ax_coreperi, all_keyword_df, example_word_coreperi, \n",
    "                                     rng=rng, near_p=near_p, radius=radius, exponent=exponent)\n",
    "\n",
    "for ax in axes_scatter.tolist() + axes_binomial.tolist() + [ax_coreperi]:\n",
    "    ax.set_xlabel(r'$n_\\mathrm{all}$', labelpad=0.8, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.set_ylabel(r'$n_w$', labelpad=0.8, fontsize=LARGE_FONT_SIZE)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=SMALL_FONT_SIZE, pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(figdir / 'all_tweets_vs_toponym_subsample_example.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
